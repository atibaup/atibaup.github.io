---
layout: post
title: "Stability of solving structured linear systems via Sherman-Morrison-Woodbury's identity"
date: 2017-03-07
---

In this blog post I consider the problem of using the Sherman-Morrison-Woodbury identity to solve linear systems of equations with a specific structure. In particular, I'm interested in linear systems where the defining matrix can be expressed as the sum of an "easy-to-solve-for" matrix plus a low-rank matrix:

$\mathbf{B} = \mathbf{A} + \mathbf{L}\mathbf{L}^T$,

These systems arise in many applications, either as an approximation to another problem, or as a consequence of the structure of the problem itself. For example, I encountered such systems when programming a KKT equations solver for a customized Interior Point method to solve a Revenue Management \[aka. optimal product pricing\] problem. You can find other examples of where this structure appears in Machine Learning optimization problems in [2].

In the first paragraph I didn't define what I meant by "easy-to-solve-for" matrix. An "easy-to-solve-for" matrix in this context is one where solving

$\mathbf{A} \mathbf{z} = \mathbf{w}$

is much faster than solving the original problem:

$\mathbf{B} \mathbf{x} = \mathbf{y}$.

There are many instances where this can happen. One common situation is when $\mathbf{A}$ is diagonal or can be re-ordered into a sparse matrix with a small band, in which case the complexity of applying an elimination-type method grows with the size of the band, instead of the size of the matrix itself.


The Sherman-Morrison-Woodbury magic \[aka. matrix inversion lemma\]
---------------------------------------------------------------------------

The Sherman-Morrisson-Woodbury \[SMW\] identity states that the inverse of the matrix

$\mathbf{B} = \mathbf{A} + \mathbf{L}\mathbf{D}\mathbf{L}^T$,

provided that $\mathbf{A}$ and $\mathbf{C} := \mathbf{D}^{-1} + \mathbf{L}^T\mathbf{A}^{-1}\mathbf{L}$ are invertible, can be computed as:

$\mathbf{B}^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{L} \mathbf{C}^{-1} \mathbf{L}^T \mathbf{A}^{-1}$

This suggests an algorithm to solve the system of equations:

$\mathbf{B} \mathbf{x} = \mathbf{y}$

in 5 steps [1]:

1. Solve $\mathbf{A}\mathbf{x}^1 = \mathbf{y}$ to find $\mathbf{x}^1$
2. Solve $\mathbf{A}\mathbf{W} = \mathbf{L}$ for $\mathbf{W}$
3. Form $\mathbf{C} = \mathbf{D}^{-1} + \mathbf{L}^T\mathbf{W}$
4. Solve $\mathbf{C} \mathbf{x}^2 = \mathbf{L}^T\mathbf{x}^1$ to find $\mathbf{x}^2$
5. Set $\mathbf{x} = \mathbf{x}^1 - \mathbf{W}\mathbf{x}^2$

If the structure of the matrix $\mathbf{A}$ is such that step (1) and (2) can be efficiently computed \[e.g. $\mathbf{A}$ is sparse\], then it may be computationally advantageous to use these 5 steps instead of directly solving $\mathbf{B} \mathbf{x} = \mathbf{y}$ \[which can be a dense system even if $\mathbf{A}$ is sparse\]. 

Unstable magic, though
----------------------

The main problem with the 5 step SWM approach is that for ill-conditioned $\mathbf{A}$ and/or $\mathbf{C}$, the numerical errors resulting from applying each step can compound meanly relative to directly solving $\mathbf{B}\mathbf{x} = \mathbf{y}$.

For example, if the conditioning of $\mathbf{A}$ is a lot poorer than $\mathbf{B}$'s, results from classic perturbation theory [3] suggest that the numerical errors introduced during the computation of Step 1 \[calculating $\mathbf{x}^1$\] could already be larger than those introduced when solving the original system $\mathbf{B}\mathbf{x} = \mathbf{y}$. In this situation, and given that $\mathbf{x}^1$ is the first summand to form the solution in Step 5, it would be reasonable to expect the resulting $\mathbf{x}$ to be of worse quality than that obtained via the direct approach.

Another well-known problematic situation is that when the matrix $\mathbf{C}$ is also ill-conditioned [1, 4].

Under these two scenarios, it is perfectly possible that $\mathbf{B}$ is still well conditioned, and hence, the SMW approach is numericaly unstable while the direct approach is fine. In a way, one can think of the term $\mathbf{L}\mathbf{L}^T$ as a "regularization" of the matrix $\mathbf{A}$. 

I've seen this phenomenon happen in practice, and it always struck me as a strange "coincidence". In the next section I'll explore the mechanism by which this "regularization" occurs in a simplified setting.

Low-rank regularization?
-------------------------

Since this is not a mathematics blog, but an engineering one, I will make a few simplifying assumptions that match the situations I've found in practice. So, for the rest of this post I'll assume that $\mathbf{A}$ is a positive definite matrix and $\mathbf{D}$ is the identity, and I will also assume that all non-zero eigenvalues of $\mathbf{A}$ and $\mathbf{L}\mathbf{L}^T$ have algebraic multiplicity of 1.

The $\ell_2$-norm condition number of a non-singular matrix $\mathbf{X}$ is defined as [3]:

$\kappa_2 \left(\mathbf{X}\right) := \left\|\left\|\mathbf{X}\right\|\right\|_2\left\|\left\|\mathbf{X}^{-1}\right\|\right\|_2 $

For p.d. matrices, this can be simplified to:

$\kappa_2 \left(\mathbf{X}\right) := \frac{\bar{\lambda}\left(\mathbf{X}\right)}{\underline{\lambda}\left(\mathbf{X}\right)}$

where $\bar{\lambda}$ and $\underline{\lambda}$ denote the largest and the smallest eigenvalue, respectively. So if we want to understand $\kappa_2 \left(\mathbf{B}\right)$ relative to $\kappa_2 \left(\mathbf{A}\right)$, we can just focus on understanding the behavior of $\bar{\lambda}\left(\mathbf{B}\right)$, $\underline{\lambda}\left(\mathbf{B}\right)$ relative to $\bar{\lambda}\left(\mathbf{A}\right)$, $\underline{\lambda}\left(\mathbf{A}\right)$.

Let's focus on $\underline{\lambda}$ first, and denote by $v_i\left(\mathbf{X}\right)$ the unit-norm eigenvector corresponding to the $i$-th eigenvalue of an $N \times N$ matrix $\mathbf{X}$, in increasing order. We have that:

$\underline{\lambda}\left(\mathbf{B}\right) = v_0\left(\mathbf{B}\right)^T \mathbf{B} v_0\left(\mathbf{B}\right) = v_0\left(\mathbf{B}\right)^T \mathbf{A} v_0\left(\mathbf{B}\right) + v_0\left(\mathbf{B}\right)^T \mathbf{L}\mathbf{L}^T v_0\left(\mathbf{B}\right)$

Define the subspace 

${S}^k = \mbox{span} \left(\{v_o\left(\mathbf{A}\right), \cdots, v_{k-1}\left(\mathbf{A}\right)\}\right)$ 

and the corresponding projection operators $$\mathbf{P}_{S}^{\parallel}$$, 
$$\mathbf{P}^{\perp}_{S}$$ onto the subspace and its orthogonal complement. We can express $v_0\left(\mathbf{B}\right)$ as the sum of its projections on the subspace and its complement:

$$v_0\left(\mathbf{B}\right) = \mathbf{P}^{\parallel}_{S} v_0\left(\mathbf{B}\right) + \mathbf{P}^{\perp}_{S} v_0\left(\mathbf{B}\right) = v^{\parallel} + v^{\perp}$$

And so it follows that:

$\underline{\lambda}\left(\mathbf{B}\right) = \left(v^{\parallel} + v^{\perp}\right)^T \mathbf{A} \left(v^{\parallel} + v^{\perp}\right) + \left(v^{\parallel} + v^{\perp}\right)^T \mathbf{L}\mathbf{L}^T \left(v^{\parallel} + v^{\perp}\right)$

$\underline{\lambda}\left(\mathbf{B}\right) = v{^{\parallel}}^T \mathbf{A} v^{\parallel} + {v^{\perp}}^T \mathbf{A} v^{\perp}  + v{^{\parallel}}^T \mathbf{L}\mathbf{L}^T v^{\parallel} + {v^{\perp}}^T \mathbf{L}\mathbf{L}^T v^{\perp} +  2 {v^{\parallel}}^T \mathbf{L}\mathbf{L}^T v^{\perp}$

Assume now that $\mathbf{L} \in S^k$ \[by which I mean that each column of $\mathbf{L}$ lives in the subspace $S^k$\]. Then $\mathbf{L}^T v^{\perp} = \mathbf{0}$, and it follows that:

$\underline{\lambda}\left(\mathbf{B}\right) = v{^{\parallel}}^T \mathbf{A} v^{\parallel} + {v^{\perp}}^T \mathbf{A} v^{\perp}  + v{^{\parallel}}^T \mathbf{L}\mathbf{L}^T v^{\parallel}$

$$\underline{\lambda}\left(\mathbf{B}\right) \geq \left\| v^{\parallel}  \right\|_2^2 \left( \lambda_{0} \left(\mathbf{A}\right) +  \lambda_{N-r} \left(\mathbf{L}\mathbf{L}^T\right)\right) +  \left\| v^{\perp} \right\|_2^2  \lambda_{k} \left(\mathbf{A}\right)$$

where $r$ is the rank of $\mathbf{L}\mathbf{L}^T$. But since we have that, by construction:

$$\left\| v_0 \left(\mathbf{B}\right)\right\|_2^2 = \left\| v^{\perp} \right\|_2^2  + \left\| v^{\parallel}  \right\|_2^2 = 1$$,

it follows that:

$\underline{\lambda}\left(\mathbf{B}\right) \geq \min\left(\lambda_{0} \left(\mathbf{A}\right) +  \lambda_{N-r} \left(\mathbf{L}\mathbf{L}^T\right),  \lambda_{k} \left(\mathbf{A}\right)\right) > \lambda_{0} \left(\mathbf{A}\right)$

We have thus found one way to "regularize" the condition number of $\mathbf{B}$ relative to that of $\mathbf{A}$ by increasing its smallest eigenvalue via a low-rank perturbation aligned with the subspace spanned by the smallest eigenvalues of $\mathbf{A}$. Not surprisingly, the amount of regularization is both a function of the perturbation as well as the larger eigenvalues of $\mathbf{A}$.

Since the $\ell_2$ condition number also depends on $\bar{\lambda}$ it is natural to ask whether a similar effect can be 


$\bar{\lambda}\left(\mathbf{B}\right) = v{^{\parallel}}^T \mathbf{A} v^{\parallel} + {v^{\perp}}^T \mathbf{A} v^{\perp}  + v{^{\parallel}}^T \mathbf{L}\mathbf{L}^T v^{\parallel}$

$$\bar{\lambda}\left(\mathbf{B}\right) \leq \left\| v^{\parallel}  \right\|_2^2 \left( \lambda_{k - 1} \left(\mathbf{A}\right) +  \lambda_{N} \left(\mathbf{L}\mathbf{L}^T\right)\right) +  \left\| v^{\perp} \right\|_2^2  \lambda_{N} \left(\mathbf{A}\right)$$

Hence

$$\bar{\lambda}\left(\mathbf{B}\right) \leq \max \left( \lambda_{k - 1} \left(\mathbf{A}\right) +  \lambda_{N} \left(\mathbf{L}\mathbf{L}^T\right), \lambda_{N} \left(\mathbf{A}\right)\right)$$


References
----------

[1] [Updating the inverse of a Matrix, W. W. Hager, SIAM Review Vol 31, No 2, pp 221-239, June 1989](http://web.tecnico.ulisboa.pt/mcasquilho/acad/or/ftp/1989SIAM_Hager.pdf)

[2]  [Interior-point methods for large-scale cone programming, M. Andersen et al., Chapter 3 in Optimization for Machine Learning, 2012](http://www.seas.ucla.edu/~vandenbe/publications/mlbook.pdf)

[3] Sections 2.2 and 2.3 of "Matrix Perturbation Theory", Stewart & Sun, 1990, Academic Press

[4] A note on the stability of solving a rank-p modification of a linear system by the Sherman-Morrison-Woodbury formula, E. L. YI, SIAM J. Sci. Statist. Comput., 7, 1986, pp. 507-513.